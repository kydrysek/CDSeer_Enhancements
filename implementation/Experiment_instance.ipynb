{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1293dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install setuptools wheel -U\n",
    "# !pip install torch -U\n",
    "# !pip install scikit-multiflow\n",
    "# !pip install setuptools\n",
    "# !pip install tqdm\n",
    "# !pip install kneefinder\n",
    "# %pip install numpy_indexed\n",
    "# %pip install river\n",
    "# %pip install python_nameof\n",
    "# %pip install jsonpickle\n",
    "# !pip install capymoa\n",
    "# %pip install shap\n",
    "# %pip install yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc362f",
   "metadata": {},
   "source": [
    "## 1. Imports and low level con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63290d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy_indexed as npi\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "import river\n",
    "from river.drift.binary import DDM, EDDM\n",
    "from river.drift import ADWIN, PageHinkley\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "from ExperimentControlFlow import *\n",
    "\n",
    "num_cpus = os.cpu_count(); print(f\"Number of CPUs: {num_cpus}\")\n",
    "np.__version__, pd.__version__, time.strftime(\"%D %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca00d9",
   "metadata": {},
   "source": [
    "## 2. Execute experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a620635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WARM_UP_WINDOW_SIZE = 1000    # this actually will have to vary by model tested\n",
    "INSPECTOR_WINDOW_SIZE = 500  # this is treated as a hyperparameter\n",
    "MEMORY_SIZE = 15        # this is treated as a hyperparameter\n",
    "\n",
    "results_of_runs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_params_list_standard = [(\"SHAP_noScale_noModel\",(True,True,True,False,False)),(\"SHAP_noScale_Model\",(True,True,True,False,True))]\n",
    "\n",
    "# cust_params_list_noSHAP = [(\"NoCluster_noScale\",(False,False,False,False,False)),(\"DBSCAN_Scale_noModel\",(False,False,True,True,False)),\n",
    "#     (\"DBSCAN_noScale_Model\",(False,False,True,False,True)),(\"DBSCAN_Scale_Model\",(False,False,True,True,True))]    \n",
    "# cust_params_list_SHAP_noPoint = [(\"SHAP_noScale_noModel\",(True,False,True,False,False)),(\"SHAP_Scale_noModel\",(True,False,True,True,False)),\n",
    "#     (\"SHAP_noScale_Model\",(True,False,True,False,True)),(\"SHAP_Scale_Model\",(True,False,True,True,True))]\n",
    "# cust_params_list_SHAP_Point = [(\"SHAP_noScale_noModel\",(True,True,True,False,False)),(\"SHAP_Scale_noModel\",(True,True,True,True,False)),\n",
    "#     (\"SHAP_noScale_Model\",(True,True,True,False,True)),(\"SHAP_Scale_Model\",(True,True,True,True,True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc26466",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = \"../datasets/chocolate/\"\n",
    "\n",
    "path_drift_set = [\n",
    "    ('nonlinear_abrupt_chocolaterotation_multi.csv',[5000,10000,15000,20000]),\n",
    "    ('nonlinear_abrupt_chocolaterotation_noise_and_redunce_binary.csv',[4800,9600,14400,19200]),\n",
    "    ('nonlinear_sudden_chocolaterotation_binary.csv',[5000,10000,15000,20000]),\n",
    "    ('nonlinear_sudden_chocolaterotation_noise_multi.csv',[4800,9600,14400,19200])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf8047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.ctime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d2194",
   "metadata": {},
   "source": [
    "# Solely supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a227a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute ad-hoc experiment \n",
    "DO_AVG=True\n",
    "limit_size=75000\n",
    "oracle_compliance_list=[.0015]\n",
    "inspector_curiosity = .01\n",
    "run_count=5\n",
    "\n",
    "seeds = list(range(1000))\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "if DO_AVG:\n",
    "    # Before running the experiment, check disk space\n",
    "    import shutil\n",
    "    total, used, free = shutil.disk_usage(\".\")\n",
    "    print(f\"Disk space: {free // (2**20)} MB free\")\n",
    "    if free < 100 * 2**20:  # less than 100MB free\n",
    "        raise OSError(\"Not enough disk space to run the experiment. Please free up space and try again.\")\n",
    "    \n",
    "    \n",
    "    res_out1 = []\n",
    "\n",
    "    # model_reference = RandomForestClassifier(random_state=42)\n",
    "    model_object = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    for prefix, conf in cust_params_list_standard:\n",
    "        for train_forward in [True,False]:\n",
    "            for oracle_compliance in oracle_compliance_list:\n",
    "                for path, drift_golden_source in path_drift_set:                \n",
    "                    full_path = path_dir+path\n",
    "                    drift_detector = river.drift.PageHinkley()\n",
    "                    clustering_params_list={\"weight\":1,\"use_SHAP_to_cluster\":conf[0],\"combine_SHAP_with_point\":conf[1],\n",
    "                                            \"perform_clustering\":conf[2],\"scale_for_clustering\":conf[3],\"use_model_pred\":conf[4]}\n",
    "                    \n",
    "                \n",
    "                    run_config = {\"file_path\":full_path,\"training_size\":WARM_UP_WINDOW_SIZE,\"memory_size\":MEMORY_SIZE,\"insp_window_size\":INSPECTOR_WINDOW_SIZE,\n",
    "                                \"cluster_method\":None,\"model_object\":model_object, \"drift_detector\":drift_detector, \"keep_known_labels\":False,\n",
    "                                \"inspector_curiosity_factor\":inspector_curiosity, \"oracle_compliance\":oracle_compliance, \"inspector_model\":RandomForestClassifier(n_estimators=100,random_state=42),#KNeighborsClassifier()#\n",
    "                                \"clustering_params_list\":clustering_params_list, \"train_forward\":train_forward, \"run_quiet\":False, \"dont_propagate\":False}\n",
    "                    \n",
    "                    forw_back_pref = (\"forw\" if run_config[\"train_forward\"] else \"back\")\n",
    "\n",
    "                    outcomes = combined_run(run_config,max_size=limit_size,skip_initial=0,prefix=f\"{limit_size}_{forw_back_pref}_{prefix}\",repetitions=run_count,random_states=seeds[:run_count],save=True)\n",
    "\n",
    "                    drift_golden_source_capped = [drift for drift in drift_golden_source if drift < LIMIT_DATASET_SIZE]\n",
    "                    agg_res = score_outcomes(outcomes,drift_golden_source,skip_initial=0,train_forward=run_config[\"train_forward\"],train_size=WARM_UP_WINDOW_SIZE,mem_size=MEMORY_SIZE)            \n",
    "                    agg_res[\"files\"] = [outcome[0] for outcome in outcomes]\n",
    "                    print(f\"\\ncumultive results: {agg_res}\")\n",
    "                    \n",
    "                    saved_file = serialise_aggregate_results(run_config,agg_res,f\"{forw_back_pref}_{prefix}\")\n",
    "                    print(f\"aggregate_file={saved_file}\\n\\n\")\n",
    "                    res_out1 += [saved_file]\n",
    "            \n",
    "            \n",
    "    print(f\"Results gathered\\n{res_out1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bfefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_results_to_df(res_out1,save_as_name=\"SHAP_4.csv\", rescore_drift=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d2663",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
